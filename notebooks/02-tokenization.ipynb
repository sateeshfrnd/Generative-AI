{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization \n",
    "\n",
    "- Tokenization is the process of breaking down a text into smaller units called tokens. These tokens can be words, subwords, or characters, depending on the tokenization method. \n",
    "- Tokenization is a key step in Natural Language Processing (NLP) tasks because it converts raw text into a structured format that models can understand.\n",
    "\n",
    "## Types of Tokenization:\n",
    "1. Word Tokenization: Splits text into words.\n",
    "   - *Example: \"I love Python\" → [\"I\", \"love\", \"Python\"]*\n",
    "\n",
    "2. Character Tokenization: Breaks text into individual characters. \n",
    "   - *Example: \"Python\" → [\"P\", \"y\", \"t\", \"h\", \"o\", \"n\"]*\n",
    "\n",
    "3. Subword Tokenization: Breaks words into smaller meaningful subunits, often used in transformer models like BERT and GPT. \n",
    "   - *Example: \"unbelievable\" → [\"un\", \"believable\"] or [\"un\", \"##believable\"]*\n",
    "\n",
    "## Importance:\n",
    "- Tokenization allows the model to handle text of varying lengths.\n",
    "- It helps preserve the meaning of the text when processing for NLP models.\n",
    "\n",
    "Common libraries for tokenization include **spaCy**, **NLTK**, and **Hugging Face's tokenizers**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr. Satish website is https://bigdataplaybook.wordpress.com, and Satish's email is sateeshfrnd@gmail.com, and he said, 'I'm visting kudramuk this weekend!'.Its takes an overnight journey of 335km from Bangalore to reach here. \n"
     ]
    }
   ],
   "source": [
    "corpus = \"Mr. Satish website is https://bigdataplaybook.wordpress.com, and Satish's email is sateeshfrnd@gmail.com, and he said, 'I'm visting kudramuk this weekend!'.Its takes an overnight journey of 335km from Bangalore to reach here. \"\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\satee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Downloading the required resources for tokenization\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Punkt Tokenizer: It is a pre-trained model that NLTK uses for tokenizing text into sentences and words. It is particularly useful because it can handle a variety of punctuation marks and abbreviations, making it effective for multiple languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize into sentences\n",
    "from nltk.tokenize import sent_tokenize\n",
    "documents=sent_tokenize(corpus, language=\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr. Satish website is https://bigdataplaybook.wordpress.com, and Satish's email is sateeshfrnd@gmail.com, and he said, 'I'm visting kudramuk this weekend!\n",
      "'.Its takes an overnight journey of 335km from Bangalore to reach here.\n"
     ]
    }
   ],
   "source": [
    "for sentence in documents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mr.',\n",
       " 'Satish',\n",
       " 'website',\n",
       " 'is',\n",
       " 'https',\n",
       " ':',\n",
       " '//bigdataplaybook.wordpress.com',\n",
       " ',',\n",
       " 'and',\n",
       " 'Satish',\n",
       " \"'s\",\n",
       " 'email',\n",
       " 'is',\n",
       " 'sateeshfrnd',\n",
       " '@',\n",
       " 'gmail.com',\n",
       " ',',\n",
       " 'and',\n",
       " 'he',\n",
       " 'said',\n",
       " ',',\n",
       " \"'\",\n",
       " 'I',\n",
       " \"'m\",\n",
       " 'visting',\n",
       " 'kudramuk',\n",
       " 'this',\n",
       " 'weekend',\n",
       " '!',\n",
       " \"'.Its\",\n",
       " 'takes',\n",
       " 'an',\n",
       " 'overnight',\n",
       " 'journey',\n",
       " 'of',\n",
       " '335km',\n",
       " 'from',\n",
       " 'Bangalore',\n",
       " 'to',\n",
       " 'reach',\n",
       " 'here',\n",
       " '.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize into words\n",
    "from nltk.tokenize import word_tokenize\n",
    "words = word_tokenize(corpus)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mr',\n",
       " '.',\n",
       " 'Satish',\n",
       " 'website',\n",
       " 'is',\n",
       " 'https',\n",
       " '://',\n",
       " 'bigdataplaybook',\n",
       " '.',\n",
       " 'wordpress',\n",
       " '.',\n",
       " 'com',\n",
       " ',',\n",
       " 'and',\n",
       " 'Satish',\n",
       " \"'\",\n",
       " 's',\n",
       " 'email',\n",
       " 'is',\n",
       " 'sateeshfrnd',\n",
       " '@',\n",
       " 'gmail',\n",
       " '.',\n",
       " 'com',\n",
       " ',',\n",
       " 'and',\n",
       " 'he',\n",
       " 'said',\n",
       " ',',\n",
       " \"'\",\n",
       " 'I',\n",
       " \"'\",\n",
       " 'm',\n",
       " 'visting',\n",
       " 'kudramuk',\n",
       " 'this',\n",
       " 'weekend',\n",
       " \"!'.\",\n",
       " 'Its',\n",
       " 'takes',\n",
       " 'an',\n",
       " 'overnight',\n",
       " 'journey',\n",
       " 'of',\n",
       " '335km',\n",
       " 'from',\n",
       " 'Bangalore',\n",
       " 'to',\n",
       " 'reach',\n",
       " 'here',\n",
       " '.']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "wordpunct_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**word_tokenize:**\n",
    "- Handles abbreviations like Mr. and email addresses more naturally, keeping parts of the email and abbreviation together.\n",
    "- Treats possessives better, like \"Satish's,\" handling them as single tokens.\n",
    "\n",
    "**wordpunct_tokenize:**\n",
    "- Breaks at every punctuation mark, including periods, apostrophes, and email addresses.\n",
    "- It separates abbreviations (like \"Mr.\" becomes ['Mr', '.']) and email addresses into individual components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When to Use:**\n",
    "- Use word_tokenize if you want a more natural breakdown of text with better handling of contractions, abbreviations, and email addresses.\n",
    "- Use wordpunct_tokenize if you need to strictly split on every punctuation mark, like when processing technical documents or tasks where punctuation needs to be isolated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TreebankWordTokenizer\n",
    "The TreebankWordTokenizer in NLTK is a tokenizer that uses the Penn Treebank conventions to tokenize text. This tokenizer is more sophisticated than basic tokenizers because it is designed to split contractions, punctuation, and special characters based on linguistic rules rather than simple whitespace or punctuation splitting.\n",
    "\n",
    "Key Features of TreebankWordTokenizer:\n",
    "- Handles contractions by splitting them into separate tokens (e.g., \"don't\" becomes [\"do\", \"n't\"]).\n",
    "- Separates punctuation from words (e.g., periods, commas, etc.).\n",
    "- Handles quotes and brackets intelligently.\n",
    "- Based on the rules used in the Penn Treebank corpus, making it a good option for parsing natural language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mr.',\n",
       " 'Satish',\n",
       " 'website',\n",
       " 'is',\n",
       " 'https',\n",
       " ':',\n",
       " '//bigdataplaybook.wordpress.com',\n",
       " ',',\n",
       " 'and',\n",
       " 'Satish',\n",
       " \"'s\",\n",
       " 'email',\n",
       " 'is',\n",
       " 'sateeshfrnd',\n",
       " '@',\n",
       " 'gmail.com',\n",
       " ',',\n",
       " 'and',\n",
       " 'he',\n",
       " 'said',\n",
       " ',',\n",
       " \"'I\",\n",
       " \"'m\",\n",
       " 'visting',\n",
       " 'kudramuk',\n",
       " 'this',\n",
       " 'weekend',\n",
       " '!',\n",
       " \"'.Its\",\n",
       " 'takes',\n",
       " 'an',\n",
       " 'overnight',\n",
       " 'journey',\n",
       " 'of',\n",
       " '335km',\n",
       " 'from',\n",
       " 'Bangalore',\n",
       " 'to',\n",
       " 'reach',\n",
       " 'here',\n",
       " '.']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer=TreebankWordTokenizer()\n",
    "tokenizer.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparison to word_tokenize:**\n",
    "- TreebankWordTokenizer is more rule-based and adheres closely to linguistic conventions (e.g., contraction handling).\n",
    "- word_tokenize is more generalized but can still handle most common text processing needs effectively.\n",
    "\n",
    "**When to Use TreebankWordTokenizer:**\n",
    "- When you need more linguistically accurate tokenization (e.g., dealing with formal text, such as parsing text from academic papers or linguistic corpora).\n",
    "- When you want to handle contractions and punctuation according to Treebank conventions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
